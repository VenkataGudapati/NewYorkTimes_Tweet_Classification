{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "capstone.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KMh8XBQ3JFx",
        "outputId": "362238e1-e3eb-418b-9d6a-0fe66b2fed97"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import datetime\n",
        "import dateutil\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from dateutil.relativedelta import relativedelta\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from keras.models import load_model\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmcXXl1z3bao",
        "outputId": "e966dbbb-b32e-4223-e46a-c4412264b7da"
      },
      "source": [
        "drive.mount('/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNo9FU_63eX6",
        "outputId": "10a4887d-1805-48c8-c3c5-aaf225acd9b7"
      },
      "source": [
        "end = datetime.date.today()\n",
        "start = end - relativedelta(years=2)\n",
        "print(end, start)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-13 2019-05-13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuKyJiyw3fkd",
        "outputId": "4454351a-6294-4b9c-e9bd-a1caf851ff02"
      },
      "source": [
        "months_in_range = [x.split(' ') for x in pd.date_range(start, end, freq='MS').strftime(\"%Y %-m\").tolist()]\n",
        "print(months_in_range)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['2019', '6'], ['2019', '7'], ['2019', '8'], ['2019', '9'], ['2019', '10'], ['2019', '11'], ['2019', '12'], ['2020', '1'], ['2020', '2'], ['2020', '3'], ['2020', '4'], ['2020', '5'], ['2020', '6'], ['2020', '7'], ['2020', '8'], ['2020', '9'], ['2020', '10'], ['2020', '11'], ['2020', '12'], ['2021', '1'], ['2021', '2'], ['2021', '3'], ['2021', '4'], ['2021', '5']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_0nNC5N3h30"
      },
      "source": [
        "def send_request(date):\n",
        "    '''Sends a request to the NYT Archive API for given date.'''\n",
        "    base_url = 'https://api.nytimes.com/svc/archive/v1/'\n",
        "    url = base_url  + date[0] + '/' + date[1] + '.json?api-key=' + '5bBmcpCW4fOGtnBURGCXLoVFo887iwWX'\n",
        "    response = requests.get(url).json()\n",
        "    time.sleep(6)\n",
        "    return response\n",
        "\n",
        "\n",
        "def is_valid(article, date):\n",
        "    '''An article is only worth checking if it is in range, and has a headline.'''\n",
        "    is_in_range = date > start and date < end\n",
        "    has_headline = type(article['headline']) == dict and 'main' in article['headline'].keys()\n",
        "    return is_in_range and has_headline\n",
        "\n",
        "\n",
        "def parse_response(response):\n",
        "    '''Parses and returns response as pandas data frame.'''\n",
        "    data = {'headline': [],  \n",
        "        'date': [], \n",
        "        'doc_type': [],\n",
        "        'material_type': [],\n",
        "        'section': [],\n",
        "        'news_desk':[],\n",
        "        'abstract':[],\n",
        "        'keywords': [],\n",
        "        'lead_paragraph':[],\n",
        "        'snippet':[]}\n",
        "    \n",
        "    articles = response['response']['docs'] \n",
        "\n",
        "    for article in articles: \n",
        "        date = dateutil.parser.parse(article['pub_date']).date()\n",
        "        if is_valid(article, date):\n",
        "            data['date'].append(date)\n",
        "            data['headline'].append(article['headline']['main']) \n",
        "            if 'section_name' in article:\n",
        "                data['section'].append(article['section_name'])\n",
        "            else:\n",
        "                data['section'].append(None)\n",
        "            data['doc_type'].append(article['document_type'])\n",
        "            if 'type_of_material' in article: \n",
        "                data['material_type'].append(article['type_of_material'])\n",
        "            else:\n",
        "                data['material_type'].append(None)\n",
        "            if 'abstract' in article: \n",
        "                data['abstract'].append(article['abstract'])\n",
        "            else:\n",
        "                data['abstract'].append(None)\n",
        "            if 'news_desk' in article: \n",
        "                data['news_desk'].append(article['news_desk'])\n",
        "            else:\n",
        "                data['news_desk'].append(None)\n",
        "            if 'lead_paragraph' in article: \n",
        "                data['lead_paragraph'].append(article['lead_paragraph'])\n",
        "            else:\n",
        "                data['lead_paragraph'].append(None)\n",
        "            if 'snippet' in article: \n",
        "                data['snippet'].append(article['snippet'])\n",
        "            else:\n",
        "                data['snippet'].append(None)\n",
        "            keywords = [keyword['value'] for keyword in article['keywords'] if keyword['name'] == 'subject']\n",
        "            new_keywords = listToString(keywords)\n",
        "            data['keywords'].append(new_keywords)\n",
        "    return pd.DataFrame(data) \n",
        "\n",
        "def listToString(s):  \n",
        "    str1 = \"\"  \n",
        "    for ele in s:  \n",
        "        str1 += (ele+\",\")  \n",
        "    return str1\n",
        "\n",
        "def word_cloud(text,color):\n",
        "    wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=color).generate(text)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def get_data(dates):\n",
        "    '''Sends and parses request/response to/from NYT Archive API for given dates.'''\n",
        "    total = 0\n",
        "    print('Date range: ' + str(dates[0]) + ' to ' + str(dates[-1]))\n",
        "    if not os.path.exists('headlines'):\n",
        "        os.mkdir('headlines')\n",
        "    for date in dates:\n",
        "        response = send_request(date)\n",
        "        df = parse_response(response)\n",
        "        total += len(df)\n",
        "        df.to_csv('headlines/' + date[0] + '-' + date[1] + '.csv', index=False)\n",
        "        print('Saving headlines/' + date[0] + '-' + date[1] + '.csv...')\n",
        "    print('Number of articles collected: ' + str(total))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower().replace('\\n',' ').replace('\\r','').strip()\n",
        "    text = re.sub(' +',' ',text)\n",
        "    text = re.sub(r'[^\\w\\s]','',text)\n",
        "    return text\n",
        "\n",
        "def tokenize_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_token = word_tokenize(text)\n",
        "    filter_sentence = [w for w in word_token if not w in stop_words]\n",
        "    text = ' '.join(filter_sentence)\n",
        "    return text\n",
        "\n",
        "\n",
        "def stemming(messages):\n",
        "    ps = PorterStemmer()\n",
        "    corpus = []\n",
        "    for i in range(0, len(messages)):\n",
        "      print(i)\n",
        "      review = re.sub('[^a-zA-Z]', ' ', messages[i])\n",
        "      review = review.lower()\n",
        "      review = review.split()\n",
        "\n",
        "      review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
        "      review = ' '.join(review)\n",
        "      corpus.append(review)\n",
        "    return corpus  \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1H5OtLFR3lJA",
        "outputId": "90c5ec2a-fcca-4337-ce07-1f24ac429575"
      },
      "source": [
        "get_data(months_in_range)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Date range: ['2019', '6'] to ['2021', '5']\n",
            "Saving headlines/2019-6.csv...\n",
            "Saving headlines/2019-7.csv...\n",
            "Saving headlines/2019-8.csv...\n",
            "Saving headlines/2019-9.csv...\n",
            "Saving headlines/2019-10.csv...\n",
            "Saving headlines/2019-11.csv...\n",
            "Saving headlines/2019-12.csv...\n",
            "Saving headlines/2020-1.csv...\n",
            "Saving headlines/2020-2.csv...\n",
            "Saving headlines/2020-3.csv...\n",
            "Saving headlines/2020-4.csv...\n",
            "Saving headlines/2020-5.csv...\n",
            "Saving headlines/2020-6.csv...\n",
            "Saving headlines/2020-7.csv...\n",
            "Saving headlines/2020-8.csv...\n",
            "Saving headlines/2020-9.csv...\n",
            "Saving headlines/2020-10.csv...\n",
            "Saving headlines/2020-11.csv...\n",
            "Saving headlines/2020-12.csv...\n",
            "Saving headlines/2021-1.csv...\n",
            "Saving headlines/2021-2.csv...\n",
            "Saving headlines/2021-3.csv...\n",
            "Saving headlines/2021-4.csv...\n",
            "Saving headlines/2021-5.csv...\n",
            "Number of articles collected: 108445\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOWA__9n3m_Y"
      },
      "source": [
        "os.chdir(\"/content/headlines\")\n",
        "extension = 'csv'\n",
        "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
        "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
        "combined_csv.to_csv( \"combined_csv.csv\", index=False, encoding='utf-8-sig')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O96vs-8x3q63",
        "outputId": "7d81c3bb-02b7-49eb-b206-4836605ad0a9"
      },
      "source": [
        "data = pd.read_csv('/content/headlines/combined_csv.csv')\n",
        "data.head()\n",
        "len(data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "108445"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOuSCgJ84KXU"
      },
      "source": [
        "sections_abstract_count = data.groupby('section')['abstract'].nunique()\n",
        "print(sections_abstract_count)\n",
        "print(len(sections_abstract_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFJ5wams5Ms3"
      },
      "source": [
        "data['section'].replace('T Magazine', 'Magazine', inplace = True) \n",
        "data['section'].replace(['Movies', 'Arts', 'Theater'], 'Entertainment', inplace = True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NagKuxHw4QsB"
      },
      "source": [
        "data = data[data.groupby('section').section.transform('count')>=2000].copy()\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "tLoVHI-R5PCQ",
        "outputId": "7d0bdaa4-ead2-424c-d2d2-71a4e7a481ab"
      },
      "source": [
        "sections_abstract_count1 = data.groupby('section')['abstract'].nunique()\n",
        "sections_abstract_count1\n",
        "len(data['section'].value_counts())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-134638b37e9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msections_abstract_count1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'abstract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msections_abstract_count1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKU5r_uO5cgP"
      },
      "source": [
        "data.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZDzvghL5e7r"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d89uQ-jz5g36"
      },
      "source": [
        "data.dropna(subset=['headline','material_type', 'keywords', 'snippet','abstract','news_desk','lead_paragraph'],axis=0,inplace=True)\n",
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25fn6ZAv5lHe"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(20,15)})\n",
        "sns.countplot(data.section)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKpp5Ri_5mpZ"
      },
      "source": [
        "data['news_length'] = data['headline'].str.len() + data['abstract'].str.len() + data['lead_paragraph'].str.len() + data['keywords'].str.len()\n",
        "data['news_length']\n",
        "data['news_length'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn5s5QkW59Jh"
      },
      "source": [
        "data['text'] = data['headline'] + \" \"+ data['abstract'] + \" \"+ data['lead_paragraph'] +\" \"+ data['keywords']\n",
        "data['text'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMJtZcRD5-PL"
      },
      "source": [
        "sns.set()\n",
        "_ = plt.hist(data['news_length'],bins=70)\n",
        "_ = plt.xlabel(\"length\")\n",
        "_ = plt.ylabel(\"count\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fm6GSdlc5_yB"
      },
      "source": [
        "categories = data['section'].unique()\n",
        "i = 1\n",
        "for category in categories:\n",
        "  subset = data[data.section == category]\n",
        "  sns.set(rc={'figure.figsize':(12,10)})\n",
        "  text = subset.abstract.values + subset.headline.values + subset.lead_paragraph.values + subset.keywords.values + subset.news_desk.values\n",
        "  word = ' '.join(text)\n",
        "  print('\\n' + str(i) + '. ' + category.upper() + '\\n')\n",
        "  if (i % 2 == 0):\n",
        "    word_cloud(word,'white')\n",
        "  else:\n",
        "    word_cloud(word,'black')\n",
        "  i = i + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_Mr0tC26DnU"
      },
      "source": [
        "section_codes = {'U.S.': 0,\n",
        "'Entertainment': 1,   \n",
        "'World': 2,\n",
        "'Opinion': 3, \n",
        "'Business Day': 4, \n",
        "'Sports': 5,\n",
        "'New York': 6,\n",
        "'Books': 7,\n",
        "'Style': 8,\n",
        "'Magazine': 9,\n",
        "'Food': 10,\n",
        "'Real Estate': 11,\n",
        "'Briefing': 12}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3bLCIKR6w6C"
      },
      "source": [
        "data['section_code'] = data['section']\n",
        "data = data.replace({'section_code':section_codes})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB7ylZL46y-1"
      },
      "source": [
        "print(len(data))\n",
        "y = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
        "for x in y:\n",
        "  print(data[data['section_code'] == x])\n",
        "  x +=1\n",
        "print(len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9rAgI4v7AzO"
      },
      "source": [
        "lstm_data = data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1LdIEq4679e"
      },
      "source": [
        "voc_size = 1500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdx7Fi817GIA"
      },
      "source": [
        "X_train, X_test, y_train,y_test = train_test_split(lstm_data['text'],lstm_data['section_code'],test_size = 0.2,random_state=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnWhDMqu7O1t"
      },
      "source": [
        "messages = lstm_data['text'].copy()\n",
        "messages = messages.reset_index(drop= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf6_gP6Q7Xbt"
      },
      "source": [
        "corpus = stemming(messages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIGRx_V77hRN"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyo8fm9V7kH_"
      },
      "source": [
        "totalLne = len(corpus)\n",
        "i = 0\n",
        "maxLen = 0\n",
        "while i < totalLne:\n",
        "  currentLen = len(corpus[i])\n",
        "  if currentLen > maxLen:\n",
        "    maxLen = currentLen\n",
        "  i += 1\n",
        "print(maxLen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIFuTwUH7l1P"
      },
      "source": [
        "onehot_repr=[one_hot(words,voc_size)for words in corpus] \n",
        "onehot_repr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPzVYiuX7oVW"
      },
      "source": [
        "sent_length=1600\n",
        "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
        "print(embedded_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT6Jb-cr7qLK"
      },
      "source": [
        "train_label = data['section_code']\n",
        "len(train_label.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnVp9lCj7rwB"
      },
      "source": [
        "train_label = train_label.reset_index(drop= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zypv_MCt7s_0"
      },
      "source": [
        "print(len(train_label), len(corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztzBvt0s7uso"
      },
      "source": [
        "embedding_dim = 64\n",
        "model = tf.keras.Sequential([\n",
        "                              tf.keras.layers.Embedding(voc_size, embedding_dim),\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim, return_sequences=True)),\n",
        "                              tf.keras.layers.Conv1D(128, 5, activation='relu', input_shape = (None, 128, 1)),\n",
        "                              tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                              tf.keras.layers.Dense(64,activation= 'relu'),\n",
        "                              tf.keras.layers.Dropout(0.3),\n",
        "                              tf.keras.layers.Dense(embedding_dim,activation= 'relu'),\n",
        "                              tf.keras.layers.Dropout(0.3),\n",
        "                              tf.keras.layers.Dense(13, activation= 'softmax')\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVgoRFyj75uC"
      },
      "source": [
        "import numpy as np\n",
        "X_final=(embedded_docs)\n",
        "y_final=(train_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joTDAoOt7-vH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_final, y_final, test_size=0.33, random_state=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08PdLSjM8GN0"
      },
      "source": [
        "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=8,batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8LZJCNt8I_k"
      },
      "source": [
        "model.save(\"BidirectionalLSTM.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSgX2Oi-8VnW"
      },
      "source": [
        "Implementing ML models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDcOqYIq8VXx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsr2c58G8VEk"
      },
      "source": [
        "#splitting data into test & train\n",
        "X_train, X_test, y_train,y_test = train_test_split(data['parsed_text'],data['section_code'],test_size = 0.2,random_state=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7joeoe48iYR"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc65qQsc8jrA"
      },
      "source": [
        "ngram_range = (1,2)\n",
        "min_df = 10\n",
        "max_df = 1.\n",
        "max_features = 30000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edg-WtYP8lHH"
      },
      "source": [
        "tfidf = TfidfVectorizer(encoding='utf-8',\n",
        "                        ngram_range = ngram_range,\n",
        "                        stop_words = None,\n",
        "                        lowercase = False,\n",
        "                        max_df = max_df,\n",
        "                        min_df = min_df,\n",
        "                        max_features = max_features,\n",
        "                        norm = 'l2',\n",
        "                        sublinear_tf = True\n",
        "                        )\n",
        "train_features = tfidf.fit_transform(X_train).toarray()\n",
        "train_label = y_train\n",
        "\n",
        "test_features = tfidf.transform(X_test).toarray()\n",
        "test_label = y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnzEwCd78mon"
      },
      "source": [
        "print(test_label.shape)\n",
        "print(train_label.shape)\n",
        "print(train_features.shape)\n",
        "print(test_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGrz3ksJ8vyc"
      },
      "source": [
        "##Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwXyac0P8obP"
      },
      "source": [
        "import pickle\n",
        "model = RandomForestClassifier()\n",
        "model.fit(train_features,train_label)\n",
        "filename = 'finalized_model'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "predictions = model.predict(test_features)\n",
        "print(accuracy_score(test_label,predictions))\n",
        "print(classification_report(test_label,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ83PNiX813L"
      },
      "source": [
        "##Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa87ZjHY81df"
      },
      "source": [
        "import pickle\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "model.fit(train_features,train_label)\n",
        "filename = 'logit_model.pkl'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        "predictions = model.predict(test_features)\n",
        "print(accuracy_score(test_label,predictions))\n",
        "print(classification_report(test_label,predictions))\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7pAAW2388lh"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(train_features,train_label)\n",
        "predictions = model.predict(test_features)\n",
        "print(accuracy_score(test_label,predictions))\n",
        "print(classification_report(test_label,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMhBDukX8-ZM"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "model = GaussianNB()\n",
        "model.fit(train_features,train_label)\n",
        "predictions = model.predict(test_features)\n",
        "print(accuracy_score(test_label,predictions))\n",
        "print(classification_report(test_label,predictions))\n",
        "print(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En5BWkfP8-QC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}